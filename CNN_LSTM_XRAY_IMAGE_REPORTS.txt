import os
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import pandas as pd
import re

# Step 1: Preprocess the data

class XRayDataset(Dataset):
    def __init__(self, img_folder, report_folder, transform=None):
        self.img_folder = img_folder
        self.report_folder = report_folder
        self.transform = transform
        self.img_names = os.listdir(img_folder)
        self.report_dict = self._load_reports()

    def _load_reports(self):
        report_dict = {}
        for file_name in os.listdir(self.report_folder):
            with open(os.path.join(self.report_folder, file_name), 'r') as file:
                report_text = file.read()
                report_dict[file_name.split('.')[0]] = report_text
        return report_dict

    def __len__(self):
        return len(self.img_names)

    def __getitem__(self, idx):
        img_name = self.img_names[idx]
        img_path = os.path.join(self.img_folder, img_name)
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        report_id = img_name.split('.')[0]
        report_text = self.report_dict[report_id]
        return image, report_text

# Step 2: Define the CNN model

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.resnet = models.resnet18(pretrained=True)
        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 512)

    def forward(self, images):
        features = self.resnet(images)
        return features

# Step 3: Define the LSTM model

class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, vocab_size):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.embedding = nn.Embedding(vocab_size, input_size)
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, features, captions):
        captions = self.embedding(captions)
        inputs = torch.cat((features.unsqueeze(1), captions), 1)
        h0 = torch.zeros(self.num_layers, inputs.size(0), self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers, inputs.size(0), self.hidden_size).to(device)
        out, _ = self.lstm(inputs, (h0, c0))
        out = self.fc(out)
        return out

# Step 4: Combine the CNN and LSTM models

class ImageCaptioningModel(nn.Module):
    def __init__(self, cnn, lstm):
        super(ImageCaptioningModel, self).__init__()
        self.cnn = cnn
        self.lstm = lstm

    def forward(self, images, captions):
        features = self.cnn(images)
        outputs = self.lstm(features, captions)
        return outputs

# Step 5: Train the combined model

# Define hyperparameters
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
input_size = 512  # Size of CNN output
hidden_size = 512  # Size of LSTM hidden state
num_layers = 1  # Number of layers in LSTM
output_size = vocab_size  # Size of vocabulary for report generation
vocab_size = 1000  # Dummy value, update with actual vocab size
learning_rate = 0.001
num_epochs = 10
batch_size = 64

# Initialize models and data loaders
cnn = CNN().to(device)
lstm = LSTM(input_size, hidden_size, num_layers, output_size, vocab_size).to(device)
model = ImageCaptioningModel(cnn, lstm).to(device)

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

dataset = XRayDataset(img_folder='./NLMCXR_png', report_folder='./NLMCXR_reports', transform=transform)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for images, captions in dataloader:
        images = images.to(device)
        captions = captions.to(device)

        optimizer.zero_grad()
        outputs = model(images, captions)
        loss = criterion(outputs.view(-1, outputs.shape[2]), captions.view(-1))
        loss.backward()
        optimizer.step()

        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')
